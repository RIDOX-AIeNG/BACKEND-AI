{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e68d54",
   "metadata": {},
   "source": [
    "# **DATA PREPROCESSING BASED ON EDA INSIGHTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ec5a0a",
   "metadata": {},
   "source": [
    "This notebook implements preprocessing steps based on the comprehensive EDA findings and recommendations. We'll follow the evidence-based approach from the EDA report to ensure our preprocessing aligns with the data patterns discovered.\n",
    "Based on the EDA report, we will:\n",
    "\n",
    "1. **Handle Skewed Variables** - Log-transform `Applicants Income`, `Co Applicants Income`, \n",
    "2. **Outlier Treatment** - IQR-capping for extreme acidity/sulphates \n",
    "3. **Feature Engineering** - Create acidity ratios and interaction features\n",
    "4. **Feature Selection** - Keep high-signal features, evaluate low-signal ones\n",
    "5. **Scaling** - StandardScaler for distance-based models\n",
    "6. **Target Handling** - Classification approach with stratified splits\n",
    "\n",
    "<!-- **Key EDA Evidence to Implement**\n",
    "\n",
    "- **High-signal features**: `alcohol`, `volatile acidity`, `sulphates`, `citric acid`, `density`, `chlorides`\n",
    "- **Low-signal features**: `residual sugar`, `free sulfur dioxide` (evaluate for removal)\n",
    "- **Skewed variables**: `residual sugar`, `total sulfur dioxide`, `chlorides` (log-transform)\n",
    "- **Feature engineering**: Acidity ratios, alcohol-acidity interactions, fermentation efficiency -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cbb0ee",
   "metadata": {},
   "source": [
    "#### **1. Import Libraries and Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1fc95e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Preprocessing libraries\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Statistical libraries\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore, skew\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3959a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the dataset\n",
    "\n",
    "url1 = r\"https://raw.githubusercontent.com/ek-chris/Practice_datasets/refs/heads/main/home_loan_train.csv\"\n",
    "url2 = r\"https://raw.githubusercontent.com/kenstare/Practice_datasets/master/home_loan_test.csv\"\n",
    "train_data = pd.read_csv(url1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d15335b",
   "metadata": {},
   "source": [
    "#### **2. EDA-Based Data Quality Assessment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554dfc22",
   "metadata": {},
   "source": [
    "Based on EDA findings lets access the issues identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de5eacb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create a copy of the data for preprocessing\n",
    "\n",
    "t_processed = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef94fe6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001003</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001005</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LP001006</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2583</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LP001008</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
       "0  LP001002   Male      No          0      Graduate            No   \n",
       "1  LP001003   Male     Yes          1      Graduate            No   \n",
       "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
       "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
       "4  LP001008   Male      No          0      Graduate            No   \n",
       "\n",
       "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "0             5849                0.0         NaN             360.0   \n",
       "1             4583             1508.0       128.0             360.0   \n",
       "2             3000                0.0        66.0             360.0   \n",
       "3             2583             2358.0       120.0             360.0   \n",
       "4             6000                0.0       141.0             360.0   \n",
       "\n",
       "   Credit_History Property_Area  Loan_Status  \n",
       "0             1.0         Urban            1  \n",
       "1             1.0         Rural            0  \n",
       "2             1.0         Urban            1  \n",
       "3             1.0         Urban            1  \n",
       "4             1.0         Urban            1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55dfd437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Loan_ID', 'Gender', 'Married', 'Dependents', 'Education',\n",
       "       'Self_Employed', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n",
       "       'Loan_Amount_Term', 'Credit_History', 'Property_Area', 'Loan_Status'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_processed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d31f064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert alpha to numeric values in the target column\n",
    "t_processed['Loan_Status'] = t_processed['Loan_Status'].map({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51f85bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1. Missing Values:\n",
      "Gender              13\n",
      "Married              3\n",
      "Dependents          15\n",
      "Self_Employed       32\n",
      "LoanAmount          22\n",
      "Loan_Amount_Term    14\n",
      "Credit_History      50\n",
      "dtype: int64\n",
      "\n",
      " 2. Duplicate Rows:\n",
      "Number of duplicate rows: 0\n",
      "\n",
      " 3. Skewness Analysis( EDA identified right-skewed varaiables):\n",
      "ApplicantIncome: skewness = 6.524 (right-skewed)\n",
      "CoapplicantIncome: skewness = 7.473 (right-skewed)\n",
      "LoanAmount: skewness = nan (approximately normal)\n",
      "\n",
      " 4. correlation with Quality (EDA Evidence):\n",
      "High-signal features (|correlation| > 0.2):\n",
      "  Credit_History: 0.562\n",
      "\n",
      "Low-signal features (|correlation| < 0.1):\n",
      "  CoapplicantIncome: -0.059\n",
      "  LoanAmount: -0.037\n",
      "  Loan_Amount_Term: -0.021\n",
      "  ApplicantIncome: -0.005\n"
     ]
    }
   ],
   "source": [
    "# LETS check for missing values\n",
    "\n",
    "print(\"\\n 1. Missing Values:\")\n",
    "missing_values = t_processed.isnull().sum()\n",
    "if missing_values.sum() > 0:\n",
    "    print(missing_values[missing_values > 0])\n",
    "else:\n",
    "    print(\"No missing values found \")\n",
    "\n",
    "# 2. check for duplicated data\n",
    "print(\"\\n 2. Duplicate Rows:\")\n",
    "duplicates = t_processed.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(f\"Percentage of duplicates: {(duplicates/len(t_processed))*100:.2f}%\")\n",
    "\n",
    "# 3. check skewness for variables identified in EDA as right skewed\n",
    "print(\"\\n 3. Skewness Analysis( EDA identified right-skewed varaiables):\")\n",
    "skewed_vars = ['ApplicantIncome','CoapplicantIncome','LoanAmount']\n",
    "for var in skewed_vars:\n",
    "    if var in t_processed.columns:\n",
    "        skewness = skew(t_processed[var])\n",
    "        print(f\"{var}: skewness = {skewness:.3f} ({'right-skewed' if skewness > 0.5 else 'approximately normal'})\")\n",
    "\n",
    "#4. Check correlation with target (EDA evidence)\n",
    "print(\"\\n 4. correlation with Quality (EDA Evidence):\")\n",
    "correlations = t_processed.select_dtypes(include=['number']).corr()['Loan_Status'].sort_values(key=abs, ascending=False)\n",
    "\n",
    "print(\"High-signal features (|correlation| > 0.2):\")\n",
    "high_signal = correlations[abs(correlations) > 0.2].drop('Loan_Status')\n",
    "for feature, corr in high_signal.items():\n",
    "    print(f\"  {feature}: {corr:.3f}\")\n",
    "\n",
    "print(\"\\nLow-signal features (|correlation| < 0.1):\")\n",
    "low_signal = correlations[abs(correlations) < 0.1]\n",
    "for feature, corr in low_signal.items():\n",
    "    print(f\"  {feature}: {corr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d8f60b",
   "metadata": {},
   "source": [
    "#### **3. Handle Duplicates**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5d50640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ No duplicates to remove (as expected from EDA)\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates if any (EDA didn't report duplicates, but let's be thorough)\n",
    "if duplicates > 0:\n",
    "    print(f\"Removing {duplicates} duplicate rows...\")\n",
    "    df_processed = t_processed.drop_duplicates()\n",
    "    print(f\"Dataset shape after removing duplicates: {t_processed.shape}\")\n",
    "else:\n",
    "    print(\"✓ No duplicates to remove (as expected from EDA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b7cd34",
   "metadata": {},
   "source": [
    "#### **4. Log-Transform Skewed Variables (EDA Recommendation)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f3acb8",
   "metadata": {},
   "source": [
    "Based on EDA findings, transform the right-skewed variables identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "597ed61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOG-TRANSFORMING SKEWED VARIABLES===\n",
      "EDA identified these variables as right-skewed and recommended log transformation:\n",
      "ApplicantIncome: Applied log transformation\n",
      "Original skewness: 6.524-> Transformed skewness: 0.478 \n",
      "CoapplicantIncome: Applied log1p transformstion (had 0.000 minimum value)\n",
      "Original skewness: 7.473-> Transformed skewness: -0.173 \n",
      "LoanAmount: Applied log transformation\n",
      "Original skewness: nan-> Transformed skewness: nan \n",
      "\n",
      " Dataset shape after log transformation: (614, 16)\n",
      "New log-transformed columns: ['ApplicantIncome_log', 'CoapplicantIncome_log', 'LoanAmount_log']\n"
     ]
    }
   ],
   "source": [
    "# Log-transform skewed varaiables as recommended by EDA\n",
    "print(\"=== LOG-TRANSFORMING SKEWED VARIABLES===\")\n",
    "print(\"EDA identified these variables as right-skewed and recommended log transformation:\")\n",
    "\n",
    "# Variables to log-transform based on EDA findings \n",
    "skewed_vars = ['ApplicantIncome','CoapplicantIncome','LoanAmount']\n",
    "\n",
    "for var in skewed_vars:\n",
    "    if var in t_processed.columns:\n",
    "        # check if variable has zero or negative values \n",
    "\n",
    "        min_val = t_processed[var].min()\n",
    "        if min_val <= 0:\n",
    "            # Use Log1p for variables with zeros\n",
    "            t_processed[f'{var}_log'] = np.log1p(t_processed[var])\n",
    "            print(f\"{var}: Applied log1p transformstion (had {min_val:.3f} minimum value)\")\n",
    "        else:\n",
    "            # Use log for variables with zeros\n",
    "            t_processed[f\"{var}_log\"] = np.log(t_processed[var])\n",
    "            print(f\"{var}: Applied log transformation\")\n",
    "\n",
    "        #check skewness before and after\n",
    "        original_skew = skew(t_processed[var])\n",
    "        transformed_skew= skew(t_processed[f'{var}_log'])\n",
    "        print(f\"Original skewness: {original_skew:.3f}-> Transformed skewness: {transformed_skew:.3f} \")\n",
    "print(f\"\\n Dataset shape after log transformation: {t_processed.shape}\")\n",
    "print(\"New log-transformed columns:\", [col for col in t_processed.columns if \"_log\" in col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aa3886",
   "metadata": {},
   "source": [
    "#### **5. Outlier Treatment (EDA Recommendation)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af8a962",
   "metadata": {},
   "source": [
    "Based on EDA findings, handle outliers using IQR-capping method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2baa0ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OUTLIER TREATMENT (IQR-CAPPING METHOD) ===\n",
      "EDA recommended IQR-capping for extreme acidity/sulphates to preserve data points\n",
      "Treating outliers in 8 numerical features...\n",
      "✓ ApplicantIncome: Capped 50 outliers\n",
      "✓ CoapplicantIncome: Capped 18 outliers\n",
      "✓ LoanAmount: Capped 39 outliers\n",
      "✓ Loan_Amount_Term: Capped 88 outliers\n",
      "✓ Credit_History: Capped 89 outliers\n",
      "✓ ApplicantIncome_log: Capped 27 outliers\n",
      "✓ LoanAmount_log: Capped 34 outliers\n",
      "\n",
      "Total outliers capped: 345\n",
      "Dataset shape after outlier treatment: (614, 16)\n"
     ]
    }
   ],
   "source": [
    "# Outlier treatment based on EDA recommendations\n",
    "print(\"=== OUTLIER TREATMENT (IQR-CAPPING METHOD) ===\")\n",
    "print(\"EDA recommended IQR-capping for extreme acidity/sulphates to preserve data points\")\n",
    "\n",
    "# Define numerical columns (excluding target)\n",
    "numerical_cols = t_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'Loan_Status' in numerical_cols:\n",
    "    numerical_cols.remove('Loan_Status')\n",
    "\n",
    "print(f\"Treating outliers in {len(numerical_cols)} numerical features...\")\n",
    "\n",
    "# Apply IQR-capping method\n",
    "outliers_capped = 0\n",
    "for col in numerical_cols:\n",
    "    Q1 = t_processed[col].quantile(0.25)\n",
    "    Q3 = t_processed[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Count outliers before capping\n",
    "    outliers_before = ((t_processed[col] < lower_bound) | (t_processed[col] > upper_bound)).sum()\n",
    "    \n",
    "    if outliers_before > 0:\n",
    "        # Cap outliers\n",
    "        t_processed[col] = np.where(t_processed[col] < lower_bound, lower_bound, t_processed[col])\n",
    "        t_processed[col] = np.where(t_processed[col] > upper_bound, upper_bound, t_processed[col])\n",
    "        outliers_capped += outliers_before\n",
    "        print(f\"✓ {col}: Capped {outliers_before} outliers\")\n",
    "\n",
    "print(f\"\\nTotal outliers capped: {outliers_capped}\")\n",
    "print(f\"Dataset shape after outlier treatment: {t_processed.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415159ca",
   "metadata": {},
   "source": [
    "#### **6. Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95aa29e",
   "metadata": {},
   "source": [
    "Implement the specific feature engineering recommendations from the EDA report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d79f2418",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Turn continuous income values into categories (Low, Medium, High)\n",
    "# EDA rationale: helps models like Decision Trees or Random Forests interpret ranges more clearly and makes EDA results more readable.\n",
    "\n",
    "# Define bins (adjust ranges to fit your dataset distribution)\n",
    "\n",
    "bins = [0, 2500, 6000, 10000, float('inf')]\n",
    "labels = ['Low', 'Medium', 'High', 'Very High']\n",
    "\n",
    "# Create new column for income category\n",
    "t_processed['Income_Category'] = pd.cut(t_processed['ApplicantIncome'], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4627f6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Income category = Applicant Income(categorized)\n",
      "\n",
      " Income category code = Income category(encoded)\n",
      "\n",
      "Total income: Applicant Income + Coapplicant Income\n",
      "[360.  nan]\n",
      "\n",
      " loan_term_category = Loan amount Term(categorized)\n",
      "\n",
      "Dataset shape after feature engineering: (614, 21)\n",
      "New engineered features: ['ApplicantIncome_log', 'CoapplicantIncome_log', 'LoanAmount_log', 'Income_Category', 'Income_Category_Code', 'TotalIncome', 'Log_TotalIncome', 'Loan_Term_Category']\n"
     ]
    }
   ],
   "source": [
    "#1. Turn continuous income values into categories (Low, Medium, High)\n",
    "# EDA rationale: helps models like Decision Trees or Random Forests interpret ranges more clearly and makes EDA results more readable.\n",
    "\n",
    "# Define bins (adjust ranges to fit your dataset distribution)\n",
    "\n",
    "bins = [0, 2500, 6000, 10000, float('inf')]\n",
    "labels = ['Low', 'Medium', 'High', 'Very High']\n",
    "\n",
    "# Create new column for income category\n",
    "t_processed['Income_Category'] = pd.cut(t_processed['ApplicantIncome'], bins=bins, labels=labels)\n",
    "print(f\"\\n Income category = Applicant Income(categorized)\")\n",
    "\n",
    "# Encoding\n",
    "t_processed['Income_Category_Code'] = t_processed['Income_Category'].map({'Low': 1, 'Medium': 2, 'High': 3, 'Very High': 4})\n",
    "print(f\"\\n Income category code = Income category(encoded)\")\n",
    "\n",
    "\n",
    "# 2. Total Income: Applicant Income + Coapplicant Income\n",
    "#  EDA rationale: reflects the household’s earning capacity\n",
    "t_processed['TotalIncome'] = t_processed['ApplicantIncome'] + t_processed['CoapplicantIncome']\n",
    "\n",
    "# To reduce skewness\n",
    "t_processed['Log_TotalIncome'] = np.log1p(t_processed['TotalIncome'])  # log1p handles zeros safely\n",
    "\n",
    "print(\"\\nTotal income: Applicant Income + Coapplicant Income\")\n",
    "\n",
    "\n",
    "\n",
    "# 3. Ctaegorize Loan_Amount_Term (often measured in months) into interpretable categories like Short, Medium, or Long Term.\n",
    "# EDA Recommendations: Enhances model interpretability\n",
    "\n",
    "# Check unique terms first\n",
    "print(t_processed['Loan_Amount_Term'].unique())\n",
    "\n",
    "# Define categories\n",
    "def categorize_loan_term(term):\n",
    "    if term <= 180:\n",
    "        return 'Short Term'\n",
    "    elif term <= 300:\n",
    "        return 'Medium Term'\n",
    "    else:\n",
    "        return 'Long Term'\n",
    "\n",
    "t_processed['Loan_Term_Category'] = t_processed['Loan_Amount_Term'].apply(categorize_loan_term)\n",
    "\n",
    "print(f\"\\n loan_term_category = Loan amount Term(categorized)\")\n",
    "\n",
    "# Now Lets check the shape of our dataset after feature engineering\n",
    "print(f\"\\nDataset shape after feature engineering: {t_processed.shape}\")\n",
    "# Lets check the new engineered features\n",
    "print(f\"New engineered features: {[col for col in t_processed.columns if col not in train_data.columns]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
